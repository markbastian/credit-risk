{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.read_csv('../input/home-credit-default-risk/application.csv')\n",
    "app_train = all_data[all_data['TARGET'].notnull()]\n",
    "app_test = all_data[all_data['TARGET'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_model(training_data, testing_data, n_folds=5):\n",
    "    y_train = training_data['TARGET']\n",
    "\n",
    "    # Remove the feature_labels and target\n",
    "    X_train = training_data.drop(columns=['SK_ID_CURR', 'TARGET'])\n",
    "    test_ids = testing_data['SK_ID_CURR']\n",
    "    X_test = testing_data.drop(columns=['SK_ID_CURR'])\n",
    "\n",
    "    # One hot encode features\n",
    "    X_train = pd.get_dummies(X_train)\n",
    "    X_test = pd.get_dummies(X_test)\n",
    "\n",
    "    # Align the dataframes by the columns\n",
    "    X_train, X_test = X_train.align(X_test, join='inner', axis=1)\n",
    "\n",
    "    print('Training Data Shape: ', X_train.shape)\n",
    "    print('Testing Data Shape: ', X_test.shape)\n",
    "\n",
    "    # Extract feature names\n",
    "    feature_names = list(X_train.columns)\n",
    "\n",
    "    # Convert to np arrays\n",
    "    X_train = np.array(X_train)\n",
    "    X_test = np.array(X_test)\n",
    "\n",
    "    # Create the kfold object\n",
    "    k_fold = KFold(n_splits=n_folds, shuffle=True, random_state=50)\n",
    "\n",
    "    # Empty array for feature importances\n",
    "    feature_importance_values = np.zeros(len(feature_names))\n",
    "\n",
    "    # Empty array for test predictions\n",
    "    test_predictions = np.zeros(X_test.shape[0])\n",
    "\n",
    "    # Empty array for out of fold validation predictions\n",
    "    out_of_fold = np.zeros(X_train.shape[0])\n",
    "\n",
    "    # Lists for recording validation and training scores\n",
    "    valid_scores = []\n",
    "    train_scores = []\n",
    "\n",
    "    # Create the model\n",
    "    model = lgb.LGBMClassifier(boosting_type='goss', max_depth=4, num_leaves=24, n_estimators=15000, objective = 'binary',\n",
    "                               class_weight = 'balanced', learning_rate = 0.04,\n",
    "                               reg_alpha = 0.2, reg_lambda = 0.2,\n",
    "                               subsample = 0.2, n_jobs = 12, random_state = 150, subsample_for_bin=50000)\n",
    "    \n",
    "#     model = lgb.LGBMClassifier(n_estimators=10000, objective='binary',\n",
    "#                                class_weight='balanced', learning_rate=0.05,\n",
    "#                                reg_alpha=0.1, reg_lambda=0.1,\n",
    "#                                subsample=0.8, n_jobs=-1, random_state=50)\n",
    "        \n",
    "    # Iterate through each fold\n",
    "    for train_indices, valid_indices in k_fold.split(X_train):\n",
    "        # Training data for the fold\n",
    "        train_features, train_labels = X_train[train_indices], y_train[train_indices]\n",
    "        # Validation data for the fold\n",
    "        valid_features, valid_labels = X_train[valid_indices], y_train[valid_indices]\n",
    "\n",
    "        # Train the model\n",
    "#         model.fit(train_features, train_labels, eval_metric='auc',\n",
    "#                   eval_set=[(valid_features, valid_labels), (train_features, train_labels)],\n",
    "#                   eval_names=['valid', 'train'], categorical_feature='auto',\n",
    "#                   early_stopping_rounds=100, verbose=200)\n",
    "\n",
    "        model.fit(train_features, train_labels, eval_metric = 'auc',\n",
    "                  eval_set = [(valid_features, valid_labels), (train_features, train_labels)],\n",
    "                  eval_names = ['valid', 'train'], categorical_feature = 'auto',\n",
    "                  early_stopping_rounds = 130, verbose = 130)\n",
    "\n",
    "        # Record the best iteration\n",
    "        best_iteration = model.best_iteration_\n",
    "\n",
    "        # Record the feature importances\n",
    "        feature_importance_values += model.feature_importances_ / k_fold.n_splits\n",
    "\n",
    "        # Make predictions\n",
    "        test_predictions += model.predict_proba(X_test, num_iteration=best_iteration)[:, 1] / k_fold.n_splits\n",
    "\n",
    "        # Record the out of fold predictions\n",
    "        out_of_fold[valid_indices] = model.predict_proba(valid_features, num_iteration=best_iteration)[:, 1]\n",
    "\n",
    "        # Record the best score\n",
    "        valid_score = model.best_score_['valid']['auc']\n",
    "        train_score = model.best_score_['train']['auc']\n",
    "\n",
    "        valid_scores.append(valid_score)\n",
    "        train_scores.append(train_score)\n",
    "\n",
    "    # Make the submission dataframe\n",
    "    submission = pd.DataFrame({'SK_ID_CURR': test_ids, 'TARGET': test_predictions})\n",
    "\n",
    "    # Make the feature importance dataframe\n",
    "    feature_importances = pd.DataFrame({'feature': feature_names, 'importance': feature_importance_values})\n",
    "\n",
    "    # Overall validation score\n",
    "    valid_auc = roc_auc_score(y_train, out_of_fold)\n",
    "\n",
    "    # Add the overall scores to the metrics\n",
    "    valid_scores.append(valid_auc)\n",
    "    train_scores.append(np.mean(train_scores))\n",
    "\n",
    "    # Needed for creating dataframe of validation scores\n",
    "    fold_names = list(range(n_folds))\n",
    "    fold_names.append('overall')\n",
    "\n",
    "    # Dataframe of validation scores\n",
    "    metrics = pd.DataFrame({'fold': fold_names,\n",
    "                            'train': train_scores,\n",
    "                            'valid': valid_scores})\n",
    "\n",
    "    return submission, feature_importances, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Shape:  (307511, 231)\n",
      "Testing Data Shape:  (48744, 231)\n",
      "Training until validation scores don't improve for 130 rounds\n",
      "[130]\ttrain's auc: 0.768201\ttrain's binary_logloss: 0.577471\tvalid's auc: 0.75385\tvalid's binary_logloss: 0.582408\n",
      "[260]\ttrain's auc: 0.782997\ttrain's binary_logloss: 0.56203\tvalid's auc: 0.759466\tvalid's binary_logloss: 0.570661\n",
      "[390]\ttrain's auc: 0.793523\ttrain's binary_logloss: 0.55146\tvalid's auc: 0.762204\tvalid's binary_logloss: 0.56328\n",
      "[520]\ttrain's auc: 0.801878\ttrain's binary_logloss: 0.54301\tvalid's auc: 0.763206\tvalid's binary_logloss: 0.557778\n"
     ]
    }
   ],
   "source": [
    "submission, fi, metrics = do_model(app_train, app_test)\n",
    "print('Baseline metrics')\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('LGBMClassifierNewDataNathanParams.csv', index=False)\n",
    "print('Created submission file!')\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importances(df):\n",
    "    # Sort features according to importance\n",
    "    df = df.sort_values('importance', ascending = False).reset_index()\n",
    "    \n",
    "    # Normalize the feature importances to add up to one\n",
    "    df['importance_normalized'] = df['importance'] / df['importance'].sum()\n",
    "\n",
    "    # Make a horizontal bar chart of feature importances\n",
    "    plt.figure(figsize = (10, 6))\n",
    "    ax = plt.subplot()\n",
    "    \n",
    "    # Need to reverse the index to plot most important on top\n",
    "    ax.barh(list(reversed(list(df.index[:15]))), \n",
    "            df['importance_normalized'].head(15), \n",
    "            align = 'center', edgecolor = 'k')\n",
    "    \n",
    "    # Set the yticks and labels\n",
    "    ax.set_yticks(list(reversed(list(df.index[:15]))))\n",
    "    ax.set_yticklabels(df['feature'].head(15))\n",
    "    \n",
    "    # Plot labeling\n",
    "    plt.xlabel('Normalized Importance'); plt.title('Feature Importances')\n",
    "    plt.show()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fi_sorted = plot_feature_importances(fi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
